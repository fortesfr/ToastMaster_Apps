                          - For hundreds of years,
analog computers were the most\npowerful comput...
predicting eclipses, tides,\nand guiding anti-a...
Then, with the advent of\nsolid-state transistors,
                       digital computers took off.
 Now, virtually every\ncomputer we use is digital.
But today, a perfect storm of\nfactors is setti...
            for a resurgence of analog technology.
                       This is an analog computer,
and by connecting these\nwires in particular ways,
           I can program it to solve a whole range
                        of differential equations.
    For example, this setup\nallows me to simulate
            a damped mass oscillating on a spring.
So on the oscilloscope, you\ncan actually see t...
                            of the mass over time.
                       And I can vary the damping,
                           or the spring constant,
    or the mass, and we can\nsee how the amplitude
          and duration of the oscillations change.
            Now what makes this an analog computer
     is that there are no\nzeros and ones in here.
Instead, there's actually\na voltage that oscil...
     up and down exactly\nlike a mass on a spring.
             The electrical circuitry is an analog
                         for the physical problem,
                  it just takes place much faster.
     Now, if I change the\nelectrical connections,
                       I can program this computer
            to solve other differential equations,
                           like the Lorenz system,
which is a basic model of\nconvection in the at...
Now the Lorenz system is\nfamous because it was...
        of the first discovered examples of chaos.
        And here, you can see the Lorenz attractor
               with its beautiful butterfly shape.
                      And on this analog computer,
                       I can change the parameters
               and see their effects in real time.
                 So these examples illustrate some
            of the advantages of analog computers.
  They are incredibly\npowerful computing devices,
and they can complete a\nlot of computations fast.
        Plus, they don't take much power to do it.
                          With a digital computer,
           if you wanna add two eight-bit numbers,
                   you need around 50 transistors,
                  whereas with an analog computer,
                         you can add two currents,
                     just by connecting two wires.
 With a digital computer\nto multiply two numbers,
        you need on the order of 1,000 transistors
                     all switching zeros and ones,
                  whereas with an analog computer,
        you can pass a current through a resistor,
         and then the voltage across this resistor
                                will be I times R.
                                   So effectively,
         you have multiplied two numbers together.
  But analog computers also\nhave their drawbacks.
                                    For one thing,
  they are not general-purpose\ncomputing devices.
I mean, you're not gonna run\nMicrosoft Word on...
And also, since the inputs\nand outputs are con...
                       I can't input exact values.
      So if I try to repeat\nthe same calculation,
    I'm never going to get\nthe exact same answer.
Plus, think about\nmanufacturing analog computers.
            There's always gonna be some variation
                 in the exact value of components,
                     like resistors or capacitors.
                    So as a general rule of thumb,
                  you can expect about a 1% error.
            So when you think of analog computers,
you can think powerful,\nfast, and energy-effic...
but also single-purpose,\nnon-repeatable, and i...
            And if those sound like deal-breakers,
                   it's because they probably are.
               I think these are the major reasons
            why analog computers fell out of favor
      as soon as digital\ncomputers became viable.
Now, here's why analog computers\nmay be making...
                               (computers beeping)
      It all starts with\nartificial intelligence.
- [Narrator] A machine\nhas been programmed to see
                              and to move objects.
                                   - AI isn't new.
                 The term was coined back in 1956.
         In 1958, Cornell University psychologist,
           Frank Rosenblatt, built the perceptron,
designed to mimic how\nneurons fire in our brains.
So here's a basic model of how\nneurons in our ...
     An individual neuron\ncan either fire or not,
    so its level of activation\ncan be represented
                               as a one or a zero.
                           The input to one neuron
         is the output from a bunch other neurons,
             but the strength of these connections
                           between neurons varies,
     so each one can be given\na different weight.
                  Some connections are excitatory,
                    so they have positive weights,
                      while others are inhibitory,
                    so they have negative weights.
                         And the way to figure out
                whether a particular neuron fires,
   is to take the activation\nof each input neuron
                       and multiply by its weight,
                  and then add these all together.
If their sum is greater than\nsome number calle...
                            then the neuron fires,
but if it's less than that,\nthe neuron doesn't...
As input, Rosenblatt's\nperceptron had 400 phot...
                        arranged in a square grid,
                to capture a 20 by 20-pixel image.
  You can think of each\npixel as an input neuron,
with its activation being\nthe brightness of th...
                       Although strictly speaking,
     the activation should\nbe either zero or one,
we can let it take any\nvalue between zero and ...
                All of these neurons are connected
                        to a single output neuron,
               each via its own adjustable weight.
         So to see if the output neuron will fire,
you multiply the activation\nof each neuron by ...
                            and add them together.
         This is essentially a vector dot product.
If the answer is larger than\nthe bias, the neu...
                           and if not, it doesn't.
                    Now the goal of the perceptron
  was to reliably distinguish\nbetween two images,
                    like a rectangle and a circle.
                                      For example,
               the output neuron could always fire
                     when presented with a circle,
        but never when presented with a rectangle.
To achieve this, the\nperception had to be trai...
     that is, shown a series\nof different circles
and rectangles, and have its\nweights adjusted ...
         We can visualize the weights as an image,
since there's a unique weight\nfor each pixel o...
Initially, Rosenblatt set\nall the weights to z...
            If the perceptron's output is correct,
          for example, here it's shown a rectangle
               and the output neuron doesn't fire,
                 no change is made to the weights.
But if it's wrong, then\nthe weights are adjusted.
            The algorithm for updating the weights
                             is remarkably simple.
Here, the output neuron didn't\nfire when it wa...
                    because it was shown a circle.
                         So to modify the weights,
you simply add the input\nactivations to the we...
    If the output neuron\nfires when it shouldn't,
                like here, when shown a rectangle,
    well, then you subtract\nthe input activations
         from the weights, and you keep doing this
         until the perceptron correctly identifies
                          all the training images.
It was shown that this\nalgorithm will always c...
so long as it's possible\nto map the two catego...
                             into distinct groups.
                              (footsteps thumping)
     The perceptron was\ncapable of distinguishing
between different shapes,\nlike rectangles and ...
                     or between different letters.
                      And according to Rosenblatt,
it could even tell the\ndifference between cats...
                   He said the machine was capable
              of what amounts to original thought,
                       and the media lapped it up.
        The "New York Times" called the perceptron
             "the embryo of an electronic computer
that the Navy expects will\nbe able to walk, talk,
                     see, write, reproduce itself,
               and be conscious of its existence."
 - [Narrator] After training\non lots of examples,
           it's given new faces it has never seen,
and is able to successfully\ndistinguish male f...
                                   It has learned.
  - In reality, the perceptron\nwas pretty limited
                              in what it could do.
It could not, in fact,\ntell apart dogs from cats.
              This and other critiques were raised
in a book by MIT giants,\nMinsky and Papert, in...
                     And that led to a bust period
for artificial neural\nnetworks and AI in general.
                It's known as the first AI winter.
           Rosenblatt did not survive this winter.
        He drowned while sailing in Chesapeake Bay
                             on his 43rd birthday.
                             (mellow upbeat music)
 - [Narrator] The NAV Lab\nis a road-worthy truck,
         modified so that researchers or computers
     can control the vehicle\nas occasion demands.
- [Derek] In the 1980s,\nthere was an AI resurg...
  when researchers at\nCarnegie Mellon created one
                   of the first self-driving cars.
                           The vehicle was steered
   by an artificial neural\nnetwork called ALVINN.
                 It was similar to the perceptron,
except it had a hidden\nlayer of artificial neu...
                     between the input and output.
  As input, ALVINN received\n30 by 32-pixel images
                                of the road ahead.
        Here, I'm showing them as 60 by 64 pixels.
    But each of these input\nneurons was connected
via an adjustable weight to a\nhidden layer of ...
  These were each connected\nto 32 output neurons.
So to go from one layer of\nthe network to the ...
              you perform a matrix multiplication:
           the input activation times the weights.
   The output neuron with\nthe greatest activation
                    determines the steering angle.
                          To train the neural net,
                        a human drove the vehicle,
              providing the correct steering angle
                          for a given input image.
All the weights in the\nneural network were adj...
                              through the training
      so that ALVINN's output\nbetter matched that
                              of the human driver.
              The method for adjusting the weights
                        is called backpropagation,
                       which I won't go into here,
        but Welch Labs has a great series on this,
            which I'll link to in the description.
              Again, you can visualize the weights
            for the four hidden neurons as images.
      The weights are initially\nset to be random,
                       but as training progresses,
the computer learns to pick\nup on certain patt...
You can see the road markings\nemerge in the we...
Simultaneously, the output\nsteering angle coal...
                    onto the human steering angle.
    The computer drove the\nvehicle at a top speed
         of around one or two kilometers per hour.
                       It was limited by the speed
at which the computer could\nperform matrix mul...
                           Despite these advances,
        artificial neural networks still struggled
                      with seemingly simple tasks,
                 like telling apart cats and dogs.
                  And no one knew whether hardware
                    or software was the weak link.
I mean, did we have a good\nmodel of intelligence,
               we just needed more computer power?
                    Or, did we have the wrong idea
about how to make intelligence\nsystems altoget...
So artificial intelligence\nexperienced another...
                                     in the 1990s.
                                 By the mid 2000s,
most AI researchers were\nfocused on improving ...
                   But one researcher, Fei-Fei Li,
     thought maybe there was\na different problem.
            Maybe these artificial neural networks
                just needed more data to train on.
So she planned to map out\nthe entire world of ...
          From 2006 to 2009, she created ImageNet,
  a database of 1.2 million\nhuman-labeled images,
                                which at the time,
was the largest labeled image\ndataset ever con...
                            And from 2010 to 2017,
                   ImageNet ran an annual contest:
the ImageNet Large Scale\nVisual Recognition Ch...
where software programs\ncompeted to correctly ...
                              and classify images.
Images were classified into\n1,000 different ca...
                including 90 different dog breeds.
   A neural network competing\nin this competition
     would have an output\nlayer of 1,000 neurons,
        each corresponding to a category of object
                   that could appear in the image.
   If the image contains,\nsay, a German shepherd,
then the output neuron\ncorresponding to German...
               should have the highest activation.
Unsurprisingly, it turned\nout to be a tough ch...
         One way to judge the performance of an AI
is to see how often the five\nhighest neuron ac...
              do not include the correct category.
           This is the so-called top-5 error rate.
In 2010, the best performer\nhad a top-5 error ...
   of 28.2%, meaning that\nnearly 1/3 of the time,
the correct answer was not\namong its top five ...
In 2011, the error rate of\nthe best performer ...
                        a substantial improvement.
                                But the next year,
                      an artificial neural network
  from the University of\nToronto, called AlexNet,
                         blew away the competition
            with a top-5 error rate of just 16.4%.
   What set AlexNet apart\nwas its size and depth.
            The network consisted of eight layers,
                    and in total, 500,000 neurons.
                                 To train AlexNet,
60 million weights and biases\nhad to be carefu...
                      using the training database.
   Because of all the big\nmatrix multiplications,
   processing a single image\nrequired 700 million
                       individual math operations.
        So training was computationally intensive.
The team managed it by\npioneering the use of G...
                       graphical processing units,
which are traditionally used\nfor driving displ...
So they're specialized for\nfast parallel compu...
      The AlexNet paper\ndescribing their research
                                 is a blockbuster.
           It's now been cited over 100,000 times,
and it identifies the\nscale of the neural network
                            as key to its success.
It takes a lot of computation\nto train and run...
  but the improvement in\nperformance is worth it.
                 With others following their lead,
                              the top-5 error rate
             on the ImageNet competition plummeted
in the years that followed,\ndown to 3.6% in 2015.
            That is better than human performance.
             The neural network that achieved this
                        had 100 layers of neurons.
                           So the future is clear:
                We will see ever increasing demand
                  for ever larger neural networks.
        And this is a problem for several reasons:
                        One is energy consumption.
     Training a neural network\nrequires an amount
 of electricity similar\nto the yearly consumption
                              of three households.
Another issue is the so-called\nVon Neumann Bot...
           Virtually every modern digital computer
                            stores data in memory,
        and then accesses it as needed over a bus.
When performing the huge\nmatrix multiplication...
                          by deep neural networks,
                  most of the time and energy goes
          into fetching those weight values rather
              than actually doing the computation.
And finally, there are the\nlimitations of Moor...
            For decades, the number of transistors
on a chip has been doubling\napproximately ever...
                  but now the size of a transistor
               is approaching the size of an atom.
So there are some fundamental\nphysical challenges
                       to further miniaturization.
So this is the perfect\nstorm for analog comput...
     Digital computers are\nreaching their limits.
Meanwhile, neural networks\nare exploding in po...
              and a lot of what they do boils down
          to a single task: matrix multiplication.
Best of all, neural networks\ndon't need the pr...
                             of digital computers.
   Whether the neural net\nis 96% or 98% confident
                     the image contains a chicken,
  it doesn't really matter,\nit's still a chicken.
               So slight variability in components
                   or conditions can be tolerated.
                               (upbeat rock music)
  I went to an analog\ncomputing startup in Texas,
                                 called Mythic AI.
Here, they're creating analog\nchips to run neu...
And they demonstrated\nseveral AI algorithms fo...
                               - Oh, there you go.
            See, it's getting you.\n(Derek laughs)
                      Yeah.\n- That's fascinating.
- The biggest use case is\naugmented in virtual...
                 If your friend is in a different,
 they're at their house\nand you're at your house,
you can actually render each\nother in the virt...
 So it needs to really\nquickly capture your pose,
               and then render it in the VR world.
  - So, hang on, is this\nfor the metaverse thing?
    - Yeah, this is a very\nmetaverse application.
This is depth estimation\nfrom just a single we...
                      It's just taking this scene,
                   and then it's doing a heat map.
           So if it's bright, it means it's close.
          And if it's far away, it makes it black.
    - [Derek] Now all these\nalgorithms can be run
                             on digital computers,
but here, the matrix multiplication\nis actuall...
              in the analog domain.\n(light music)
                            To make this possible,
Mythic has repurposed\ndigital flash storage ce...
                 Normally these are used as memory
                  to store either a one or a zero.
If you apply a large positive\nvoltage to the c...
electrons tunnel up through\nan insulating barrier
          and become trapped on the floating gate.
                               Remove the voltage,
and the electrons can\nremain on the floating gate
for decades, preventing the\ncell from conducti...
And that's how you can store\neither a one or a...
                 You can read out the stored value
                      by applying a small voltage.
     If there are electrons\non the floating gate,
               no current flows, so that's a zero.
                        If there aren't electrons,
         then current does flow, and that's a one.
           Now Mythic's idea is to use these cells
not as on/off switches,\nbut as variable resist...
They do this by putting a\nspecific number of e...
on each floating gate,\ninstead of all or nothing.
              The greater the number of electrons,
         the higher the resistance of the channel.
             When you later apply a small voltage,
     the current that flows\nis equal to V over R.
But you can also think of this\nas voltage time...
where conductance is just\nthe reciprocal of re...
                So a single flash cell can be used
to multiply two values together,\nvoltage times...
So to use this to run an\nartificial neural net...
well they first write all the\nweights to the f...
                       as each cell's conductance.
            Then, they input the activation values
                      as the voltage on the cells.
          And the resulting current is the product
                     of voltage times conductance,
                 which is activation times weight.
        The cells are wired together in such a way
that the current from each\nmultiplication adds...
             completing the matrix multiplication.
                                     (light music)
                   - So this is our first product.
This can do 25 trillion\nmath operations per se...
                            - [Derek] 25 trillion.
   - Yep, 25 trillion math\noperations per second,
                         in this little chip here,
               burning about three watts of power.
 - [Derek] How does it\ncompare to a digital chip?
      - The newer digital\nsystems can do anywhere
   from 25 to 100 trillion\noperations per second,
         but they are big, thousand-dollar systems
  that are spitting out 50\nto 100 watts of power.
                    - [Derek] Obviously this isn't
          like an apples apples comparison, right?
                  - No, it's not apples to apples.
                I mean, training those algorithms,
                  you need big hardware like this.
   You can just do all sorts\nof stuff on the GPU,
   but if you specifically\nare doing AI workloads
and you wanna deploy 'em,\nyou could use this i...
         You can imagine them in security cameras,
                               autonomous systems,
           inspection equipment for manufacturing.
            Every time they make a Frito-Lay chip,
                    they inspect it with a camera,
and the bad Fritos get blown\noff of the convey...
         But they're using artificial intelligence
            to spot which Fritos are good and bad.
      - Some have proposed\nusing analog circuitry
                           in smart home speakers,
solely to listen for the wake\nword, like Alexa...
They would use a lot less\npower and be able to...
and reliably turn on the\ndigital circuitry of ...
But you still have to deal\nwith the challenges...
             - So for one of the popular networks,
                       there would be 50 sequences
           of matrix multiplies that you're doing.
Now, if you did that entirely\nin the analog do...
                by the time it gets to the output,
                            it's just so distorted
            that you don't have any result at all.
         So you convert it from the analog domain,
                       back to the digital domain,
             send it to the next processing block,
and then you convert it into\nthe analog domain...
      And that allows you to\npreserve the signal.
    - You know, when Rosenblatt\nwas first setting
                                up his perceptron,
                   he used a digital IBM computer.
                              Finding it too slow,
                he built a custom analog computer,
                  complete with variable resistors
                  and little motors to drive them.
           Ultimately, his idea of neural networks
                           turned out to be right.
             Maybe he was right about analog, too.
Now, I can't say whether\nanalog computers will...
             off the way digital did last century,
              but they do seem to be better suited
     to a lot of the tasks\nthat we want computers
                                 to perform today,
                       which is a little bit funny
               because I always thought of digital
    as the optimal way of\nprocessing information.
                Everything from music to pictures,
to video has all gone\ndigital in the last 50 y...
                         But maybe in a 100 years,
                     we will look back on digital,
not not as the end point\nof information techno...
                          but as a starting point.
                            Our brains are digital
     in that a neuron either\nfires or it doesn't,
                           but they're also analog
in that thinking takes place\neverywhere, all a...
                             So maybe what we need
          to achieve true artificial intelligence,
machines that think like\nus, is the power of a...
                                    (gentle music)
    Hey, I learned a lot\nwhile making this video,
much of it by playing with\nan actual analog co...
          You know, trying things out for yourself
                  is really the best way to learn,
and you can do that with this\nvideo sponsor, B...
                    Brilliant is a website and app
                     that gets you thinking deeply
               by engaging you in problem-solving.
     They have a great course\non neural networks,
    where you can test how\nit works for yourself.
               It gives you an excellent intuition
about how neural networks can\nrecognize number...
and it also allows you to\nexperience the impor...
           of good training data and hidden layers
              to understand why more sophisticated
                      neural networks work better.
                       What I love about Brilliant
             is it tests your knowledge as you go.
               The lessons are highly interactive,
  and they get progressively\nharder as you go on.
And if you get stuck, there\nare always helpful...
                        For viewers of this video,
        Brilliant is offering the first 200 people
           20% off an annual premium subscription.
              Just go to brilliant.org/veritasium.
    I will put that link\ndown in the description.
So I wanna thank Brilliant\nfor supporting Veri...
               and I wanna thank you for watching.